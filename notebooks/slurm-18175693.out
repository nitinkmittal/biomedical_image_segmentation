distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.132:32902'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.132:40260'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.132:41208'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.132:45089'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.132:33259'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.132:41782'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.132:43187'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.132:36624'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.132:44040'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.132:39793'
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.132:34463
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.132:43703
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.132:45073
distributed.worker - INFO -          Listening to:  tcp://10.99.101.132:34463
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.132:35766
distributed.worker - INFO -          Listening to:  tcp://10.99.101.132:43703
distributed.worker - INFO -          Listening to:  tcp://10.99.101.132:45073
distributed.worker - INFO -          dashboard at:        10.99.101.132:38530
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.132:38281
distributed.worker - INFO -          dashboard at:        10.99.101.132:45546
distributed.worker - INFO -          Listening to:  tcp://10.99.101.132:35766
distributed.worker - INFO -          dashboard at:        10.99.101.132:35458
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.132:37416
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.132:35341
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.132:34415
distributed.worker - INFO -          Listening to:  tcp://10.99.101.132:38281
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO -          dashboard at:        10.99.101.132:32776
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://10.99.101.132:37416
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        10.99.101.132:39067
distributed.worker - INFO -          Listening to:  tcp://10.99.101.132:35341
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO -          Listening to:  tcp://10.99.101.132:34415
distributed.worker - INFO -          dashboard at:        10.99.101.132:38036
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO -          dashboard at:        10.99.101.132:39414
distributed.worker - INFO -          dashboard at:        10.99.101.132:44095
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.132:45227
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-b3gq_mq3
distributed.worker - INFO -          Listening to:  tcp://10.99.101.132:45227
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-mei1iroc
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -          dashboard at:        10.99.101.132:35656
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-klfuifzk
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-lvxft2j_
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-6rjev4f8
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-9oh2j3m6
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-_8jgyj3q
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.132:42942
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-0hiceb21
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -          Listening to:  tcp://10.99.101.132:42942
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        10.99.101.132:35079
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-uhulejce
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-8qu0etzv
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2b9f8edd04c0>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2ad5a81ea4c0>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2b75ef0114c0>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2aca9e76b4c0>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2af378888280>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2b78ff0c04c0>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2b85730f14c0>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2b00c4c0f4c0>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2aeb3b37b4c0>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2aec63947160>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Stopping worker at tcp://10.99.101.132:37416
distributed.worker - INFO - Stopping worker at tcp://10.99.101.132:35766
distributed.worker - INFO - Stopping worker at tcp://10.99.101.132:45073
distributed.worker - INFO - Stopping worker at tcp://10.99.101.132:34415
distributed.worker - INFO - Stopping worker at tcp://10.99.101.132:34463
distributed.worker - INFO - Stopping worker at tcp://10.99.101.132:35341
distributed.worker - INFO - Stopping worker at tcp://10.99.101.132:45227
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://10.99.101.132:38281
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://10.99.101.132:42942
distributed.worker - INFO - Stopping worker at tcp://10.99.101.132:43703
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.132:40260'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.132:33259'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.132:36624'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.132:39793'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.132:32902'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.132:45089'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.132:41208'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.132:43187'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.132:44040'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.132:41782'
distributed.dask_worker - INFO - End worker
