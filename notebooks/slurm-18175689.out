distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.122:37555'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.122:39173'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.122:41317'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.122:34557'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.122:34274'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.122:38365'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.122:32819'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.122:43036'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.122:36862'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.122:38203'
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.122:44166
distributed.worker - INFO -          Listening to:  tcp://10.99.101.122:44166
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.122:42931
distributed.worker - INFO -          dashboard at:        10.99.101.122:42160
distributed.worker - INFO -          Listening to:  tcp://10.99.101.122:42931
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO -          dashboard at:        10.99.101.122:40705
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.122:37183
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -          Listening to:  tcp://10.99.101.122:37183
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.122:40823
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-1hzodnrw
distributed.worker - INFO -          dashboard at:        10.99.101.122:33213
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.122:40540
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-nbbenk01
distributed.worker - INFO -          Listening to:  tcp://10.99.101.122:40823
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://10.99.101.122:40540
distributed.worker - INFO -          dashboard at:        10.99.101.122:38381
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO -          dashboard at:        10.99.101.122:42442
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.122:42241
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-p99h45in
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -          Listening to:  tcp://10.99.101.122:42241
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-wbei4e27
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -          dashboard at:        10.99.101.122:44514
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-agzs0i36
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-6908s_34
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.122:44639
distributed.worker - INFO -          Listening to:  tcp://10.99.101.122:44639
distributed.worker - INFO -          dashboard at:        10.99.101.122:39091
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-7y6xonbm
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.122:42173
distributed.worker - INFO -          Listening to:  tcp://10.99.101.122:42173
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.122:39944
distributed.worker - INFO -          dashboard at:        10.99.101.122:35856
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://10.99.101.122:39944
distributed.worker - INFO -          dashboard at:        10.99.101.122:42281
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.122:41928
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-l77t0d6b
distributed.worker - INFO -          Listening to:  tcp://10.99.101.122:41928
distributed.worker - INFO -          dashboard at:        10.99.101.122:37475
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-egvjufgg
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-vkswrurg
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2b5ba39ea9d0>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2b019c5b7430>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2abf1f3eb4c0>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2b8a17efe4c0>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2af44fca98b0>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2ad33be1b4c0>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2b0fda1314c0>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2ae52b6b1280>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2b31e587d4c0>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2b8c4f1e84c0>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Stopping worker at tcp://10.99.101.122:37183
distributed.worker - INFO - Stopping worker at tcp://10.99.101.122:42241
distributed.worker - INFO - Stopping worker at tcp://10.99.101.122:40540
distributed.worker - INFO - Stopping worker at tcp://10.99.101.122:40823
distributed.worker - INFO - Stopping worker at tcp://10.99.101.122:44166
distributed.worker - INFO - Stopping worker at tcp://10.99.101.122:39944
distributed.worker - INFO - Stopping worker at tcp://10.99.101.122:42931
distributed.worker - INFO - Stopping worker at tcp://10.99.101.122:44639
distributed.worker - INFO - Stopping worker at tcp://10.99.101.122:42173
distributed.worker - INFO - Stopping worker at tcp://10.99.101.122:41928
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.122:39173'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.122:32819'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.122:36862'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.122:37555'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.122:34557'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.122:34274'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.122:38203'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.122:41317'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.122:43036'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.122:38365'
distributed.dask_worker - INFO - End worker
