distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.124:39775'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.124:35192'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.124:41136'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.124:44463'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.124:46088'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.124:45893'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.124:46491'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.124:33524'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.124:34618'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.124:37598'
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.124:40273
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.124:33471
distributed.worker - INFO -          Listening to:  tcp://10.99.101.124:40273
distributed.worker - INFO -          dashboard at:        10.99.101.124:42274
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://10.99.101.124:33471
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.124:38538
distributed.worker - INFO -          dashboard at:        10.99.101.124:46142
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -          Listening to:  tcp://10.99.101.124:38538
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-0bggkap2
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO -          dashboard at:        10.99.101.124:44583
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-kskeown6
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-7y5owso7
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.124:32950
distributed.worker - INFO -          Listening to:  tcp://10.99.101.124:32950
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.124:33522
distributed.worker - INFO -          dashboard at:        10.99.101.124:43272
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://10.99.101.124:33522
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -          dashboard at:        10.99.101.124:41139
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-292tzz2o
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.124:34947
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-7azcmqbl
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://10.99.101.124:34947
distributed.worker - INFO -          dashboard at:        10.99.101.124:45218
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-zrun05u4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.124:42447
distributed.worker - INFO -          Listening to:  tcp://10.99.101.124:42447
distributed.worker - INFO -          dashboard at:        10.99.101.124:37171
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-dt32qa9f
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.124:42892
distributed.worker - INFO -          Listening to:  tcp://10.99.101.124:42892
distributed.worker - INFO -          dashboard at:        10.99.101.124:39009
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-scj9v56o
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.124:46588
distributed.worker - INFO -          Listening to:  tcp://10.99.101.124:46588
distributed.worker - INFO -          dashboard at:        10.99.101.124:40379
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-3rvk86jf
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.124:34996
distributed.worker - INFO -          Listening to:  tcp://10.99.101.124:34996
distributed.worker - INFO -          dashboard at:        10.99.101.124:40858
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-tg8y41_m
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2b80ced464c0>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2ac2157dd4c0>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2b04ea0df4c0>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2aeb5b13d4c0>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2acc9fcb0430>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2aeb22c824c0>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2b4d8878c4c0>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2af8155754c0>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2aad509934c0>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2b8f33dc44c0>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Stopping worker at tcp://10.99.101.124:34996
distributed.worker - INFO - Stopping worker at tcp://10.99.101.124:33522
distributed.worker - INFO - Stopping worker at tcp://10.99.101.124:38538
distributed.worker - INFO - Stopping worker at tcp://10.99.101.124:40273
distributed.worker - INFO - Stopping worker at tcp://10.99.101.124:42892
distributed.worker - INFO - Stopping worker at tcp://10.99.101.124:46588
distributed.worker - INFO - Stopping worker at tcp://10.99.101.124:33471
distributed.worker - INFO - Stopping worker at tcp://10.99.101.124:34947
distributed.worker - INFO - Stopping worker at tcp://10.99.101.124:32950
distributed.worker - INFO - Stopping worker at tcp://10.99.101.124:42447
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.124:35192'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.124:44463'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.124:34618'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.124:41136'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.124:37598'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.124:46491'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.124:39775'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.124:45893'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.124:46088'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.124:33524'
distributed.dask_worker - INFO - End worker
