distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.33:36435'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.33:34728'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.33:34720'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.33:43995'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.33:40721'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.33:37351'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.33:33670'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.33:45551'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.33:41724'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.33:43872'
distributed.worker - INFO -       Start worker at:   tcp://10.99.101.33:44364
distributed.worker - INFO -          Listening to:   tcp://10.99.101.33:44364
distributed.worker - INFO -          dashboard at:         10.99.101.33:37544
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:34797
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-9rbfi0ye
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.99.101.33:38731
distributed.worker - INFO -          Listening to:   tcp://10.99.101.33:38731
distributed.worker - INFO -          dashboard at:         10.99.101.33:38862
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:34797
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-fokn8p1r
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.99.101.33:40078
distributed.worker - INFO -          Listening to:   tcp://10.99.101.33:40078
distributed.worker - INFO -          dashboard at:         10.99.101.33:34512
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:34797
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-egi3pqdf
distributed.worker - INFO -       Start worker at:   tcp://10.99.101.33:35410
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://10.99.101.33:35410
distributed.worker - INFO -          dashboard at:         10.99.101.33:35567
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:34797
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-acrqxbq0
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.99.101.33:37206
distributed.worker - INFO -          Listening to:   tcp://10.99.101.33:37206
distributed.worker - INFO -          dashboard at:         10.99.101.33:37785
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:34797
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-xx2ft0ss
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.99.101.33:41930
distributed.worker - INFO -          Listening to:   tcp://10.99.101.33:41930
distributed.worker - INFO -          dashboard at:         10.99.101.33:34059
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:34797
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-hyncjywn
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.99.101.33:39817
distributed.worker - INFO -          Listening to:   tcp://10.99.101.33:39817
distributed.worker - INFO -          dashboard at:         10.99.101.33:38819
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:34797
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://10.99.101.33:38090
distributed.worker - INFO -          Listening to:   tcp://10.99.101.33:38090
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -          dashboard at:         10.99.101.33:42574
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:34797
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-k_zbp0i5
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-s4p28u1r
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:34797
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:34797
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://10.99.101.33:35441
distributed.worker - INFO -          Listening to:   tcp://10.99.101.33:35441
distributed.worker - INFO -          dashboard at:         10.99.101.33:41526
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:34797
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-zrbya2lp
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:34797
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:34797
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:34797
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:34797
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:34797
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:34797
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:34797
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://10.99.101.33:33032
distributed.worker - INFO -          Listening to:   tcp://10.99.101.33:33032
distributed.worker - INFO -          dashboard at:         10.99.101.33:36473
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:34797
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-9kr94pfo
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:34797
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - ERROR - failed during get data with tcp://10.99.101.33:39817 -> None
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/iostream.py", line 867, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/iostream.py", line 1140, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 1366, in get_data
    response = await comm.read(deserializers=serializers)
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/comm/tcp.py", line 206, in read
    convert_stream_closed_error(self, e)
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/comm/tcp.py", line 124, in convert_stream_closed_error
    raise CommClosedError(
distributed.comm.core.CommClosedError: in <closed TCP>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.core - INFO - Lost connection to 'tcp://10.99.101.17:54670': in <closed TCP>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - INFO - Stopping worker at tcp://10.99.101.33:38731
distributed.worker - INFO - Stopping worker at tcp://10.99.101.33:33032
distributed.worker - INFO - Stopping worker at tcp://10.99.101.33:40078
distributed.worker - INFO - Stopping worker at tcp://10.99.101.33:38090
distributed.worker - INFO - Stopping worker at tcp://10.99.101.33:41930
distributed.worker - INFO - Stopping worker at tcp://10.99.101.33:35441
distributed.worker - INFO - Stopping worker at tcp://10.99.101.33:35410
distributed.worker - INFO - Stopping worker at tcp://10.99.101.33:37206
distributed.worker - INFO - Stopping worker at tcp://10.99.101.33:44364
distributed.worker - INFO - Stopping worker at tcp://10.99.101.33:39817
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.33:41724'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.33:40721'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.33:34720'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.33:37351'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.33:33670'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.33:36435'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.33:43995'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.33:43872'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.33:34728'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.33:45551'
distributed.dask_worker - INFO - End worker
