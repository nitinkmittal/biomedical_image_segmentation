distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.147:46058'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.147:33086'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.147:33751'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.147:41652'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.147:46124'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.147:36118'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.147:38001'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.147:36275'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.147:33512'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.99.101.147:43104'
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.147:38440
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.147:46764
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.147:42909
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.147:41933
distributed.worker - INFO -          Listening to:  tcp://10.99.101.147:46764
distributed.worker - INFO -          Listening to:  tcp://10.99.101.147:42909
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.147:34595
distributed.worker - INFO -          dashboard at:        10.99.101.147:40503
distributed.worker - INFO -          dashboard at:        10.99.101.147:42166
distributed.worker - INFO -          Listening to:  tcp://10.99.101.147:41933
distributed.worker - INFO -          Listening to:  tcp://10.99.101.147:34595
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.147:38295
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.147:41805
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO -          Listening to:  tcp://10.99.101.147:38440
distributed.worker - INFO -          dashboard at:        10.99.101.147:45024
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO -          dashboard at:        10.99.101.147:34407
distributed.worker - INFO -          Listening to:  tcp://10.99.101.147:38295
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.147:44127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://10.99.101.147:41805
distributed.worker - INFO -          dashboard at:        10.99.101.147:37175
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO -          dashboard at:        10.99.101.147:43860
distributed.worker - INFO -          dashboard at:        10.99.101.147:40904
distributed.worker - INFO -          Listening to:  tcp://10.99.101.147:44127
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO -          dashboard at:        10.99.101.147:45678
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.147:40088
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://10.99.101.147:40088
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-ip3ux_ub
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -          dashboard at:        10.99.101.147:40761
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-35m82fpc
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -       Start worker at:  tcp://10.99.101.147:35753
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-h5xwcuvh
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-p1gpnq11
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-uk5suq3r
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -          Listening to:  tcp://10.99.101.147:35753
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-0h92zl9b
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        10.99.101.147:40812
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-mr63kz6n
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://10.99.101.17:45590
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-zd4ai_8p
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-6zxbiqoj
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          5
distributed.worker - INFO -                Memory:                   9.31 GiB
distributed.worker - INFO -       Local Directory: /home/mittal.nit/projects/biomedical_image_segmentation/notebooks/dask-worker-space/worker-qcgcl_3_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://10.99.101.17:45590
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2b3cae1094c0>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2b111b5ef4c0>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2b2bb29c14c0>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2ace8cef94c0>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2b21159cc4c0>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2b108d7514c0>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2b4e083a14c0>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2b0526e244c0>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2ac38c3d74c0>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2b58b727a4c0>
Traceback (most recent call last):
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/tornado/ioloop.py", line 905, in _run
    return self.callback()
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/worker.py", line 712, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/home/mittal.nit/.conda/envs/biomedical_image_segmentation/lib/python3.9/site-packages/distributed/batched.py", line 136, in send
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Stopping worker at tcp://10.99.101.147:44127
distributed.worker - INFO - Stopping worker at tcp://10.99.101.147:40088
distributed.worker - INFO - Stopping worker at tcp://10.99.101.147:34595
distributed.worker - INFO - Stopping worker at tcp://10.99.101.147:38440
distributed.worker - INFO - Stopping worker at tcp://10.99.101.147:35753
distributed.worker - INFO - Stopping worker at tcp://10.99.101.147:38295
distributed.worker - INFO - Stopping worker at tcp://10.99.101.147:41805
distributed.worker - INFO - Stopping worker at tcp://10.99.101.147:46764
distributed.worker - INFO - Stopping worker at tcp://10.99.101.147:42909
distributed.worker - INFO - Stopping worker at tcp://10.99.101.147:41933
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.147:43104'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.147:36118'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.147:33086'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.147:46058'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.147:38001'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.147:46124'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.147:36275'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.147:41652'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.147:33512'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.99.101.147:33751'
distributed.dask_worker - INFO - End worker
