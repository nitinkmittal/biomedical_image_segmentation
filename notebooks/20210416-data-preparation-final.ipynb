{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook can be used to generate data for UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OUIfX-u-_fon"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from scipy import ndimage\n",
    "from typing import Tuple, Union, List, Any\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from pickle import dump, load\n",
    "import logging\n",
    "from time import time\n",
    "\n",
    "from dask_jobqueue import PBSCluster\n",
    "from dask.distributed import Client, get_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biomedical_image_segmentation.elastic_deform import custom_2d_elastic_deform\n",
    "from biomedical_image_segmentation.utils import insert_grid, split, create_dir, empty_dir\n",
    "from biomedical_image_segmentation.data.generator import generate\n",
    "from biomedical_image_segmentation.data.validator import validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_PATH = os.getcwd().replace(\"notebooks\",\"\")\n",
    "LOG_PATH = os.path.join(PROJECT_PATH, \"logs\")\n",
    "DATA_PATH = os.path.join(PROJECT_PATH, \"data\")\n",
    "SAMPLES_PATH = os.path.join(DATA_PATH, \"samples\")\n",
    "TRAIN_SAMPLES_PATH = os.path.join(SAMPLES_PATH, \"train\")\n",
    "MASKS_PATH = os.path.join(DATA_PATH, \"masks\")\n",
    "TRAIN_MASKS_PATH = os.path.join(MASKS_PATH, \"train\")\n",
    "\n",
    "AUGMENTED_DATA_PATH = os.path.join(DATA_PATH, \"augmented\")\n",
    "VALID_DATA_PATH = os.path.join(DATA_PATH, \"valid\")\n",
    "TEST_DATA_PATH = os.path.join(DATA_PATH, \"test\")\n",
    "\n",
    "SCRIPT_NAME = \"data-preparation\"\n",
    "LOG_FILE_NAME = os.path.join(LOG_PATH, f\"{SCRIPT_NAME}_{int(time())}.log\")\n",
    "\n",
    "RANDOM_STATE = 40,\n",
    "NUM_ELASTIC_DEFORMS = 20\n",
    "ALPHA_AFFINE = (.01, .2)\n",
    "SIGMA = 10.\n",
    "ALPHA = 1.\n",
    "ADJUSTMENT_PIXEL_RANGE = (5, 100)\n",
    "ADJUSTED_PIXEL = 0\n",
    "SPLIT_RATIO = (2/3, 1/6, 1/6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dir(LOG_PATH, True)\n",
    "\n",
    "if os.path.basename(LOG_FILE_NAME) in os.listdir(LOG_PATH): os.remove(LOG_FILE_NAME)\n",
    "    \n",
    "logging.basicConfig(\n",
    "    filename=LOG_FILE_NAME,\n",
    "    format='%(asctime)s %(message)s',\n",
    "    datefmt='%m/%d/%Y %I:%M:%S %p',\n",
    "    encoding='utf-8', \n",
    "    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Creating directory to store train dataset\")\n",
    "create_dir(AUGMENTED_DATA_PATH)\n",
    "logging.info(\"Creating directory to store valid dataset\")\n",
    "create_dir(VALID_DATA_PATH)\n",
    "logging.info(\"Creating directory to store test dataset\")\n",
    "create_dir(TEST_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f\"Removing content of {AUGMENTED_DATA_PATH}\")\n",
    "empty_dir(AUGMENTED_DATA_PATH, True)\n",
    "\n",
    "logging.info(f\"Removing content of {VALID_DATA_PATH}\")\n",
    "empty_dir(VALID_DATA_PATH, True)\n",
    "\n",
    "logging.info(f\"Removing content of {TEST_DATA_PATH}\")\n",
    "empty_dir(TEST_DATA_PATH, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare valid and test dataset before train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, valid_ids, test_ids = split(\n",
    "    os.listdir(TRAIN_SAMPLES_PATH), \n",
    "    ratio=SPLIT_RATIO, \n",
    "    seed=RANDOM_STATE)\n",
    "\n",
    "logging.info(f\"train ids: {train_ids}\")\n",
    "logging.info(f\"valid ids: {valid_ids}\")\n",
    "logging.info(f\"test ids: {test_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Preparing valid dataset\")\n",
    "dataset = []\n",
    "for i, valid_id in enumerate(valid_ids):\n",
    "    img =  cv2.imread(os.path.join(TRAIN_SAMPLES_PATH, valid_id))\n",
    "    mask = cv2.imread(os.path.join(TRAIN_MASKS_PATH, valid_id))\n",
    "    dataset.append((img[:,:,0], mask[:,:,0]))\n",
    "\n",
    "logging.info(\"Saving valid dataset\")\n",
    "with open(f\"{os.path.join(VALID_DATA_PATH, 'valid.pickle')}\", \"wb\") as f:\n",
    "    dump(dataset, f)\n",
    "    \n",
    "    \n",
    "logging.info(\"Preparing test dataset\")\n",
    "dataset = []\n",
    "for i, test_id in enumerate(test_ids):\n",
    "    img =  cv2.imread(os.path.join(TRAIN_SAMPLES_PATH, test_id))\n",
    "    mask = cv2.imread(os.path.join(TRAIN_MASKS_PATH, test_id))\n",
    "    dataset.append((img[:,:,0], mask[:,:,0]))\n",
    "\n",
    "logging.info(\"Saving test dataset\")\n",
    "with open(f\"{os.path.join(TEST_DATA_PATH, 'test.pickle')}\", \"wb\") as f:\n",
    "    dump(dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation = True\n",
    "# threshold_validation = -1.\n",
    "\n",
    "# for i, train_id in tqdm(enumerate(train_ids), leave=False):\n",
    "    \n",
    "#     img =  cv2.imread(os.path.join(TRAIN_SAMPLES_PATH, train_id))\n",
    "#     mask = cv2.imread(os.path.join(TRAIN_MASKS_PATH, train_id))\n",
    "    \n",
    "#     dataset = generate(\n",
    "#         img[:,:,0],\n",
    "#         mask[:,:,0],\n",
    "#         num_elastic_deforms=NUM_ELASTIC_DEFORMS,\n",
    "#         alpha_affine=ALPHA_AFFINE,\n",
    "#         sigma=SIGMA,\n",
    "#         alpha=ALPHA,\n",
    "#         adjustment_pixel_range=ADJUSTMENT_PIXEL_RANGE, \n",
    "#         adjusted_pixel=ADJUSTED_PIXEL)\n",
    "    \n",
    "#     if (validation and np.random.uniform() > threshold_validation): validate(dataset)\n",
    "    \n",
    "        \n",
    "#     with open(f\"{os.path.join(AUGMENTED_DATA_PATH, train_id.replace('.tif',''))}.pickle\", \"wb\") as f:\n",
    "#         dump(dataset, f)\n",
    "    \n",
    "#     del dataset\n",
    "    \n",
    "# logging.info(\"Saved train dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed: using HPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distributed_data_generation(\n",
    "    train_id, \n",
    "    img_load_path,\n",
    "    mask_load_path,\n",
    "    save_path,\n",
    "    num_elastic_deforms,\n",
    "    alpha_affine,\n",
    "    sigma,\n",
    "    alpha,\n",
    "    adjustment_pixel_range, \n",
    "    adjusted_pixel) -> bool:\n",
    "    \n",
    "    \"\"\"\n",
    "    Augment data in distributed environment.\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    status: boolean\n",
    "        Status of job, True if success otherwise False\n",
    "    \"\"\"\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    status = False\n",
    "    \n",
    "    img =  cv2.imread(os.path.join(img_load_path, train_id))\n",
    "    mask = cv2.imread(os.path.join(mask_load_path, train_id))\n",
    "    \n",
    "    dataset = generate(\n",
    "        img[:,:,0],\n",
    "        mask[:,:,0],\n",
    "        num_elastic_deforms=num_elastic_deforms,\n",
    "        alpha_affine=alpha_affine,\n",
    "        sigma=sigma,\n",
    "        alpha=alpha,\n",
    "        adjustment_pixel_range=adjustment_pixel_range, \n",
    "        adjusted_pixel=adjusted_pixel)\n",
    "    \n",
    "    try:\n",
    "        validate(dataset)\n",
    "        \n",
    "        with open(f\"{os.path.join(save_path, train_id.replace('.tif',''))}.pickle\", \"wb\") as f:\n",
    "            dump(dataset, f)\n",
    "        status = True\n",
    "    except Exception as e:\n",
    "        time()\n",
    "        get_worker().log_event(\"error\", {\"error\": e, \"status\": status})\n",
    "        \n",
    "    get_worker().log_event(\"runtimes\", {\"time elapsed\": f\"{time()-start: .5f} seconds\"})\n",
    "    \n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='background-color: #f2f2f2; display: inline-block; padding: 10px; border: 1px solid #999999;'>\n",
       "  <h3>PBSCluster</h3>\n",
       "  <ul>\n",
       "    <li><b>Dashboard: </b><a href='http://10.99.253.72:8787/status' target='_blank'>http://10.99.253.72:8787/status</a>\n",
       "  </ul>\n",
       "</div>\n"
      ],
      "text/plain": [
       "PBSCluster(6e8dc868, 'tcp://10.99.253.72:36927', workers=0, threads=0, memory=0 B)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N_WORKERS = 4 # number of parallel threads \n",
    "CORES = 16 # cores_per_worker = cores/ n_workers\n",
    "MEMORY = \"16GB\" # memory_per_worker = memory/ n_workers\n",
    "\n",
    "logging.info(\"Initializing cluster\")\n",
    "cluster = PBSCluster(\n",
    "    n_workers=N_WORKERS,\n",
    "    cores=CORES,\n",
    "    memory=MEMORY)\n",
    "logging.info(f\"Dashboard link: {cluster.dashboard_link}\")\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 446 ms, sys: 98.6 ms, total: 544 ms\n",
      "Wall time: 26.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logging.info(\"Warming up cluster\")\n",
    "def test(a, b):\n",
    "    return a + b\n",
    "results_future = {}\n",
    "for i in range(10):\n",
    "    results_future[i] = client.submit(test, a=10+i, b=10+i)\n",
    "results = client.gather(results_future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                      \r"
     ]
    }
   ],
   "source": [
    "results_future = {}\n",
    "for train_id in tqdm(train_ids, leave=False):\n",
    "    results_future[train_id] = client.submit(\n",
    "        distributed_data_generation,\n",
    "        train_id=train_id, \n",
    "        img_load_path=TRAIN_SAMPLES_PATH,\n",
    "        mask_load_path=TRAIN_MASKS_PATH,\n",
    "        save_path=AUGMENTED_DATA_PATH,\n",
    "        num_elastic_deforms=NUM_ELASTIC_DEFORMS,\n",
    "        alpha_affine=ALPHA_AFFINE,\n",
    "        sigma=SIGMA,\n",
    "        alpha=ALPHA,\n",
    "        adjustment_pixel_range=ADJUSTMENT_PIXEL_RANGE, \n",
    "        adjusted_pixel=ADJUSTED_PIXEL)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finished    20\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([results_future[i].status for i in results_future.keys()]).value_counts()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Elastic deformation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "biomedical_image_segmentation",
   "language": "python",
   "name": "biomedical_image_segmentation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
